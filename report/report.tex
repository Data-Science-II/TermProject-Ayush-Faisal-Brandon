\documentclass{article}

\usepackage{geometry}
\geometry{letterpaper, total={7in, 10in} }

\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{appendix}

\title{Term Project CSCI 4360, Housing Market Forecasting}
\author{Ayush Kumar, Faisal Hossain, Brandon Amirouche}
\date{April 27, 2021}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	
	\section{Problem Statement: Forecasting Housing Market}
	
	The goal of our project was to determine if future housing market pricing can be explained by 
	previous housing market data. We also wanted analyze the usage of exogenous variables in our time-series 
	modeling, and whether it makes a difference. This task is distinct from housing market prediction, which 
	relies on the features of the market, and the house. Using time-series data presented our group a new 
	and novel challenge. 

	\section{The Datasets}
	
	We identified two datasets for potential use in our projects. 
	
	\begin{enumerate}
		\item  \href{https://www.quandl.com/databases/ZILLOW/data}{Quandl Data from Zillow}
		\item \href{https://www.consumerfinance.gov/data-research/hmda/historic-data/?geo=nationwide&records=all-records&field_descriptions=labels}{USA Home Mortgage Disclosure Act Data}
	\end{enumerate}

	Each dataset has very different forms, and presented different challenges. The Housing Mortgage Disclosure Act was passed 
	by congress in 1975, and requires multiple federal agencies to keep track of all mortgage loans filed for and 
	denied. The datasets that we are using from the HMDA contain all mortgages filed for in America for a given year. 
	This data is very large, and we decided to use only a subset of the the available data from 2014-2017. These 
	4 datasets combined were around 40GB of data, and so we required a different approach for data analysis. We 
	got around the large nature of the data using an SQLite3 database and querying it for EDA using the corresponding 
	sqlite3 python library. 
	
	The Zillow data comes from Quandl, a web api for many types of time-series data. This data has over 70,000 regions
	and corresponds to years of aggregated monthly home sales. For the purposes of this project we downloaded 43 different 
	regional data, mostly from the state of Washington. These different regions will be used for testing our models, and we will try 
	to compare them to one another. A key drawback of this approach is that some regions have data that goes further back 
	than other regions, as long as the future housing values aren't affected by values from further in the past than 10+ years
	the results should be directly comparable. 
	
	We will decide on which datasets to use for our modeling after taking into account the preprocessing that we will need, and 
	doing some exploratory data analysis. 

	\section{Preprocessing Techniques}
	
	\subsection{Zillow Data}
	
	Different models will require different data manipulation techniques, but the Quandl curates clean easy to use datasets. For this 
	reason all our group had to do was take out the required columns, and cast them to the correct types. The columns we used for 
	our analysis were: 
	
	\begin{enumerate}
		\item date - the date of the observation, always the 1st of the month 
		\item value - the average home sell value in that particular location
		
	\end{enumerate}

	We simply cast the date to a numpy date-time object and left the values as were. For the RNN models we did rescale the data 
	using a MinMaxScaler, but we will cover that more in depth in the modeling section of the report. 
	
	\subsection{HMDA Data}
	The HMDA data is much dirtier and much more complicated to use. We applied the following filters before undergoing any 
	sort of EDA.  
	
	\begin{enumerate}
		\item Property Type - limited to one-to-four family dwellings
		\item Approved Mortgages - Houses were actually sold 
		\item loan purpose - only home purchase 
		\item state - Washington State Only because of Zillow regions
	\end{enumerate}

	Since we will be focusing on aggregated data we will simply be ignoring data that was missing. 	We aggregated the following 
	columns for further analysis, as they were based on characteristics of the region overall rather than specific to a particular 
	mortgage application. 
	
	\begin{enumerate}
		\item Loan Amount (Thousands)
		\item Population 
		\item Minority Population (Expressed as a percentage)
		\item HUD Median Family Income 
	\end{enumerate}

	\section{Exploratory Data Analysis}
	
	\includegraphics[scale = 0.5]{../plots/hmda_hist.png}
	
	\section{Modeling}
	\subsection{Auto-Regressive Model}
	\subsection{Seasonal Auto-Regressive Integrated Moving Average}
	\subsection{Gated Recurrent Unit RNN}
	\subsection{Long Short-Term Memory}
	
	LSTM uses silimar shaped data as GRU, and maintains a control flow similar as an RNN. LSTM processes the data continuously as it grows 
	forward, while the data analyst is allowed to keep or forget the previous code. The analyst may choose to use the previous data through
	operations that can allow the LSTM to keep or forget information as an instruction.

	Graphs
	
	\includegraphics[scale = 0.2]{../plots/wentachee_1d_lstm.png}
	\includegraphics[scale = 0.2]{../plots/wentachee_lag.png}

	\subsection{Results}

	The LSTM gives us an accurate prediction that is slightly better than others. The R-Squared is the
	highest given of all three models (not including the autoregression). While the GRU model tended to
	underestimate, or rather undershoot the actual, the LSTM actually overshot it. The LSTM predicted
	higher numbers than the GRU, however both could not account for the dip in the housing market that 
	we believe was caused by the coronavirus pandemic. This is a reasonable assumption since these models
	cannot predict real world issues and events, only how a model should perform under the same previous
	circumstances. The lag plot given was also similar to the GRU, and gave farely accurate measurements
	as well.
	
	\section{Results \& Conclusion}
	\section{Recommendations of Study}
	
\end{document}